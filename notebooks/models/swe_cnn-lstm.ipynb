{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#Installing the requirements that Google Colab doesn't have\n",
    "!pip install timm \n",
    "!pip install wandb --quiet\n",
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All of our imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from torchmetrics import R2Score\n",
    "\n",
    "import timm\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import progress\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in the data\n",
    "df = pd.read_csv(f'/home/ubuntu/SnowData/Final_CNN_Dataframe.csv')\n",
    "\n",
    "#Designating which columns are our metadata\n",
    "feature_cols = [col for col in df.columns \n",
    "                if col not in \n",
    "                ['cell_id', 'date', 'MOD10A1_filelocations', 'MYD10A1_filelocations', \n",
    "                 'copernicus_filelocations', 'SWE','sentinel1_filelocation','sentinel2a_filelocation',\n",
    "                 'sentinel2b_filelocation','SWE_Scaled']]\n",
    "                 #,'mean_inversed_swe', 'mean_local_swe', 'median_local_swe', 'max_local_swe', 'min_local_swe',\n",
    "                 #'mean_local_elevation', 'median_local_elevation', 'max_local_elevation', 'min_local_elevation']]\n",
    "\n",
    "#Min max scaling the meta data\n",
    "scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "df[feature_cols] =  scaler.fit_transform(df[feature_cols])\n",
    "\n",
    "#We will create a separate scaler for the targets so that we can transform them back and forth\n",
    "target_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "target_scaler.fit(np.array(df['SWE']).reshape(-1, 1))\n",
    "df['SWE_Scaled'] = target_scaler.transform(np.array(df['SWE']).reshape(-1, 1))\n",
    "\n",
    "tabluar_columns = len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tmp drop two rows until better solution i s found\n",
    "df = df.drop([56955,82314])\n",
    "df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join weather data\n",
    "a = pd.read_csv('/home/ubuntu/SnowData/Unique_CellIDs_byDate__ASO_50M_SWE_USCALB__with_HRRR_TMP_surface_12h.csv',index_col=0)\n",
    "\n",
    "b = pd.read_csv('/home/ubuntu/SnowData/Unique_CellIDs_byDate__ASO_50M_SWE_USCALB__with_HRRR_PRATE_surface_12h.csv',index_col=0)\n",
    "\n",
    "weather = pd.merge(a, b,  how='left', on = ['index','cell_id','geometry','date','month_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>cell_id</th>\n",
       "      <th>geometry</th>\n",
       "      <th>date</th>\n",
       "      <th>month_year</th>\n",
       "      <th>HRRR_TMP_surface_12h</th>\n",
       "      <th>HRRR_PRATE_surface_12h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4145875</td>\n",
       "      <td>ASO_50M_SWE_USCALB_34</td>\n",
       "      <td>POLYGON ((-119.0388715261663 37.63160426503699...</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4145876</td>\n",
       "      <td>ASO_50M_SWE_USCALB_34</td>\n",
       "      <td>POLYGON ((-119.0388715261663 37.63160426503699...</td>\n",
       "      <td>2016-04-02</td>\n",
       "      <td>2016-04</td>\n",
       "      <td>268.944534</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4145877</td>\n",
       "      <td>ASO_50M_SWE_USCALB_34</td>\n",
       "      <td>POLYGON ((-119.0388715261663 37.63160426503699...</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>2016-04</td>\n",
       "      <td>267.336797</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4145878</td>\n",
       "      <td>ASO_50M_SWE_USCALB_34</td>\n",
       "      <td>POLYGON ((-119.0388715261663 37.63160426503699...</td>\n",
       "      <td>2016-04-04</td>\n",
       "      <td>2016-04</td>\n",
       "      <td>269.976764</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4145879</td>\n",
       "      <td>ASO_50M_SWE_USCALB_34</td>\n",
       "      <td>POLYGON ((-119.0388715261663 37.63160426503699...</td>\n",
       "      <td>2016-04-05</td>\n",
       "      <td>2016-04</td>\n",
       "      <td>263.873207</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                cell_id  \\\n",
       "0  4145875  ASO_50M_SWE_USCALB_34   \n",
       "1  4145876  ASO_50M_SWE_USCALB_34   \n",
       "2  4145877  ASO_50M_SWE_USCALB_34   \n",
       "3  4145878  ASO_50M_SWE_USCALB_34   \n",
       "4  4145879  ASO_50M_SWE_USCALB_34   \n",
       "\n",
       "                                            geometry        date month_year  \\\n",
       "0  POLYGON ((-119.0388715261663 37.63160426503699...  2016-04-01    2016-04   \n",
       "1  POLYGON ((-119.0388715261663 37.63160426503699...  2016-04-02    2016-04   \n",
       "2  POLYGON ((-119.0388715261663 37.63160426503699...  2016-04-03    2016-04   \n",
       "3  POLYGON ((-119.0388715261663 37.63160426503699...  2016-04-04    2016-04   \n",
       "4  POLYGON ((-119.0388715261663 37.63160426503699...  2016-04-05    2016-04   \n",
       "\n",
       "   HRRR_TMP_surface_12h  HRRR_PRATE_surface_12h  \n",
       "0                   NaN                     NaN  \n",
       "1            268.944534                     0.0  \n",
       "2            267.336797                     0.0  \n",
       "3            269.976764                     0.0  \n",
       "4            263.873207                     0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = []\n",
    "for i in df['date'].values:\n",
    "    if pd.to_datetime(i).strftime('%Y') in year:\n",
    "        continue\n",
    "    else:\n",
    "        year.append(pd.to_datetime(i).strftime('%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2017', '2019', '2016', '2018']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>date</th>\n",
       "      <th>SWE</th>\n",
       "      <th>mean_inversed_swe</th>\n",
       "      <th>mean_local_swe</th>\n",
       "      <th>median_local_swe</th>\n",
       "      <th>max_local_swe</th>\n",
       "      <th>min_local_swe</th>\n",
       "      <th>mean_local_elevation</th>\n",
       "      <th>median_local_elevation</th>\n",
       "      <th>...</th>\n",
       "      <th>MOD10A1_Albedo</th>\n",
       "      <th>MOD10A1_NDSI</th>\n",
       "      <th>MYD10A1_SnowCover</th>\n",
       "      <th>MYD10A1_Albedo</th>\n",
       "      <th>MYD10A1_NDSI</th>\n",
       "      <th>copernicus_filelocations</th>\n",
       "      <th>sentinel1_filelocation</th>\n",
       "      <th>sentinel2a_filelocation</th>\n",
       "      <th>sentinel2b_filelocation</th>\n",
       "      <th>SWE_Scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15710</th>\n",
       "      <td>ASO_50M_SWE_USCALB_34</td>\n",
       "      <td>2016-06-26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005395</td>\n",
       "      <td>0.038814</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>0.139460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.692991</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.446482</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15739</th>\n",
       "      <td>ASO_50M_SWE_USCALB_34</td>\n",
       "      <td>2019-06-11</td>\n",
       "      <td>15.829635</td>\n",
       "      <td>0.384468</td>\n",
       "      <td>0.229136</td>\n",
       "      <td>0.277015</td>\n",
       "      <td>0.370436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.599335</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424774</td>\n",
       "      <td>0.752302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.570568</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.110631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27183</th>\n",
       "      <td>ASO_50M_SWE_USCALB_34</td>\n",
       "      <td>2018-04-22</td>\n",
       "      <td>13.872599</td>\n",
       "      <td>0.281264</td>\n",
       "      <td>0.217322</td>\n",
       "      <td>0.199527</td>\n",
       "      <td>0.270771</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.692991</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409823</td>\n",
       "      <td>0.871451</td>\n",
       "      <td>0.861655</td>\n",
       "      <td>0.400462</td>\n",
       "      <td>0.924836</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.096953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34391</th>\n",
       "      <td>ASO_50M_SWE_USCALB_34</td>\n",
       "      <td>2019-03-09</td>\n",
       "      <td>26.766102</td>\n",
       "      <td>0.505245</td>\n",
       "      <td>0.407289</td>\n",
       "      <td>0.450295</td>\n",
       "      <td>0.480332</td>\n",
       "      <td>0.012967</td>\n",
       "      <td>0.692991</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.631610</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577381</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.187064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56723</th>\n",
       "      <td>ASO_50M_SWE_USCALB_34</td>\n",
       "      <td>2016-06-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>0.004448</td>\n",
       "      <td>0.025930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.563082</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190742</td>\n",
       "      <td>0.448911</td>\n",
       "      <td>0.300449</td>\n",
       "      <td>0.203541</td>\n",
       "      <td>0.536286</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56752</th>\n",
       "      <td>ASO_50M_SWE_USCALB_34</td>\n",
       "      <td>2019-07-15</td>\n",
       "      <td>3.031035</td>\n",
       "      <td>0.161778</td>\n",
       "      <td>0.077386</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.153512</td>\n",
       "      <td>0.006797</td>\n",
       "      <td>0.712991</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095896</td>\n",
       "      <td>0.369793</td>\n",
       "      <td>0.084266</td>\n",
       "      <td>0.088460</td>\n",
       "      <td>0.306187</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.021183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63365</th>\n",
       "      <td>ASO_50M_SWE_USCALB_34</td>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>26.661538</td>\n",
       "      <td>0.544034</td>\n",
       "      <td>0.375617</td>\n",
       "      <td>0.344530</td>\n",
       "      <td>0.521080</td>\n",
       "      <td>0.010412</td>\n",
       "      <td>0.692991</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522063</td>\n",
       "      <td>0.893085</td>\n",
       "      <td>0.633250</td>\n",
       "      <td>0.483239</td>\n",
       "      <td>0.826693</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.186333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65794</th>\n",
       "      <td>ASO_50M_SWE_USCALB_34</td>\n",
       "      <td>2019-07-03</td>\n",
       "      <td>4.891785</td>\n",
       "      <td>0.170884</td>\n",
       "      <td>0.079870</td>\n",
       "      <td>0.024915</td>\n",
       "      <td>0.162163</td>\n",
       "      <td>0.007375</td>\n",
       "      <td>0.712991</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194309</td>\n",
       "      <td>0.502425</td>\n",
       "      <td>0.270051</td>\n",
       "      <td>0.177648</td>\n",
       "      <td>0.515136</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.034188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70860</th>\n",
       "      <td>ASO_50M_SWE_USCALB_34</td>\n",
       "      <td>2016-06-14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004448</td>\n",
       "      <td>0.006174</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.563082</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207232</td>\n",
       "      <td>0.461297</td>\n",
       "      <td>0.231533</td>\n",
       "      <td>0.150593</td>\n",
       "      <td>0.470380</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75932</th>\n",
       "      <td>ASO_50M_SWE_USCALB_34</td>\n",
       "      <td>2016-06-21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005170</td>\n",
       "      <td>0.002662</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.563082</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025627</td>\n",
       "      <td>0.322151</td>\n",
       "      <td>0.125694</td>\n",
       "      <td>0.071761</td>\n",
       "      <td>0.382609</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80405</th>\n",
       "      <td>ASO_50M_SWE_USCALB_34</td>\n",
       "      <td>2016-05-09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313344</td>\n",
       "      <td>0.192088</td>\n",
       "      <td>0.189078</td>\n",
       "      <td>0.302346</td>\n",
       "      <td>0.006999</td>\n",
       "      <td>0.692991</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457128</td>\n",
       "      <td>0.944641</td>\n",
       "      <td>0.927258</td>\n",
       "      <td>0.427383</td>\n",
       "      <td>0.950628</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81995</th>\n",
       "      <td>ASO_50M_SWE_USCALB_34</td>\n",
       "      <td>2017-07-18</td>\n",
       "      <td>4.414596</td>\n",
       "      <td>0.188652</td>\n",
       "      <td>0.118669</td>\n",
       "      <td>0.061645</td>\n",
       "      <td>0.189231</td>\n",
       "      <td>0.012950</td>\n",
       "      <td>0.692991</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007345</td>\n",
       "      <td>0.341561</td>\n",
       "      <td>0.074208</td>\n",
       "      <td>0.083613</td>\n",
       "      <td>0.304855</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.030853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82765</th>\n",
       "      <td>ASO_50M_SWE_USCALB_34</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>0.018351</td>\n",
       "      <td>0.167108</td>\n",
       "      <td>0.108187</td>\n",
       "      <td>0.058007</td>\n",
       "      <td>0.162477</td>\n",
       "      <td>0.012047</td>\n",
       "      <td>0.692991</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048038</td>\n",
       "      <td>0.256044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152159</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.000128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97581</th>\n",
       "      <td>ASO_50M_SWE_USCALB_34</td>\n",
       "      <td>2017-01-28</td>\n",
       "      <td>26.041847</td>\n",
       "      <td>0.448248</td>\n",
       "      <td>0.508068</td>\n",
       "      <td>0.493134</td>\n",
       "      <td>0.429999</td>\n",
       "      <td>0.414531</td>\n",
       "      <td>0.692991</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957750</td>\n",
       "      <td>0.876750</td>\n",
       "      <td>0.805389</td>\n",
       "      <td>0.882054</td>\n",
       "      <td>0.872601</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.182002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97610</th>\n",
       "      <td>ASO_50M_SWE_USCALB_34</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>2.439669</td>\n",
       "      <td>0.026155</td>\n",
       "      <td>0.038418</td>\n",
       "      <td>0.030077</td>\n",
       "      <td>0.064297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.692991</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253324</td>\n",
       "      <td>0.496176</td>\n",
       "      <td>0.268172</td>\n",
       "      <td>0.233454</td>\n",
       "      <td>0.472357</td>\n",
       "      <td>/home/ubuntu/SnowData/CopernicusData/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...</td>\n",
       "      <td>/home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...</td>\n",
       "      <td>0.017050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     cell_id        date        SWE  mean_inversed_swe  \\\n",
       "15710  ASO_50M_SWE_USCALB_34  2016-06-26   0.000000           0.005395   \n",
       "15739  ASO_50M_SWE_USCALB_34  2019-06-11  15.829635           0.384468   \n",
       "27183  ASO_50M_SWE_USCALB_34  2018-04-22  13.872599           0.281264   \n",
       "34391  ASO_50M_SWE_USCALB_34  2019-03-09  26.766102           0.505245   \n",
       "56723  ASO_50M_SWE_USCALB_34  2016-06-07   0.000000           0.026864   \n",
       "56752  ASO_50M_SWE_USCALB_34  2019-07-15   3.031035           0.161778   \n",
       "63365  ASO_50M_SWE_USCALB_34  2019-05-01  26.661538           0.544034   \n",
       "65794  ASO_50M_SWE_USCALB_34  2019-07-03   4.891785           0.170884   \n",
       "70860  ASO_50M_SWE_USCALB_34  2016-06-14   0.000000           0.006482   \n",
       "75932  ASO_50M_SWE_USCALB_34  2016-06-21   0.000000           0.005170   \n",
       "80405  ASO_50M_SWE_USCALB_34  2016-05-09   0.000000           0.313344   \n",
       "81995  ASO_50M_SWE_USCALB_34  2017-07-18   4.414596           0.188652   \n",
       "82765  ASO_50M_SWE_USCALB_34  2017-08-15   0.018351           0.167108   \n",
       "97581  ASO_50M_SWE_USCALB_34  2017-01-28  26.041847           0.448248   \n",
       "97610  ASO_50M_SWE_USCALB_34  2018-06-01   2.439669           0.026155   \n",
       "\n",
       "       mean_local_swe  median_local_swe  max_local_swe  min_local_swe  \\\n",
       "15710        0.038814          0.002118       0.139460       0.000000   \n",
       "15739        0.229136          0.277015       0.370436       0.000000   \n",
       "27183        0.217322          0.199527       0.270771       0.002503   \n",
       "34391        0.407289          0.450295       0.480332       0.012967   \n",
       "56723        0.011407          0.004448       0.025930       0.000000   \n",
       "56752        0.077386          0.028000       0.153512       0.006797   \n",
       "63365        0.375617          0.344530       0.521080       0.010412   \n",
       "65794        0.079870          0.024915       0.162163       0.007375   \n",
       "70860        0.004237          0.004448       0.006174       0.000811   \n",
       "75932        0.002662          0.001483       0.004939       0.000000   \n",
       "80405        0.192088          0.189078       0.302346       0.006999   \n",
       "81995        0.118669          0.061645       0.189231       0.012950   \n",
       "82765        0.108187          0.058007       0.162477       0.012047   \n",
       "97581        0.508068          0.493134       0.429999       0.414531   \n",
       "97610        0.038418          0.030077       0.064297       0.000000   \n",
       "\n",
       "       mean_local_elevation  median_local_elevation  ...  MOD10A1_Albedo  \\\n",
       "15710              0.692991                0.738462  ...        0.000000   \n",
       "15739              0.599335                0.692308  ...        0.424774   \n",
       "27183              0.692991                0.738462  ...        0.409823   \n",
       "34391              0.692991                0.738462  ...        0.000000   \n",
       "56723              0.563082                0.692308  ...        0.190742   \n",
       "56752              0.712991                0.738462  ...        0.095896   \n",
       "63365              0.692991                0.738462  ...        0.522063   \n",
       "65794              0.712991                0.738462  ...        0.194309   \n",
       "70860              0.563082                0.692308  ...        0.207232   \n",
       "75932              0.563082                0.692308  ...        0.025627   \n",
       "80405              0.692991                0.738462  ...        0.457128   \n",
       "81995              0.692991                0.738462  ...        0.007345   \n",
       "82765              0.692991                0.738462  ...        0.048038   \n",
       "97581              0.692991                0.738462  ...        0.957750   \n",
       "97610              0.692991                0.738462  ...        0.253324   \n",
       "\n",
       "       MOD10A1_NDSI  MYD10A1_SnowCover  MYD10A1_Albedo  MYD10A1_NDSI  \\\n",
       "15710      0.287264           0.000000        0.000000      0.446482   \n",
       "15739      0.752302           0.000000        0.000000      0.570568   \n",
       "27183      0.871451           0.861655        0.400462      0.924836   \n",
       "34391      0.631610           0.000000        0.000000      0.577381   \n",
       "56723      0.448911           0.300449        0.203541      0.536286   \n",
       "56752      0.369793           0.084266        0.088460      0.306187   \n",
       "63365      0.893085           0.633250        0.483239      0.826693   \n",
       "65794      0.502425           0.270051        0.177648      0.515136   \n",
       "70860      0.461297           0.231533        0.150593      0.470380   \n",
       "75932      0.322151           0.125694        0.071761      0.382609   \n",
       "80405      0.944641           0.927258        0.427383      0.950628   \n",
       "81995      0.341561           0.074208        0.083613      0.304855   \n",
       "82765      0.256044           0.000000        0.000000      0.152159   \n",
       "97581      0.876750           0.805389        0.882054      0.872601   \n",
       "97610      0.496176           0.268172        0.233454      0.472357   \n",
       "\n",
       "                                copernicus_filelocations  \\\n",
       "15710  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "15739  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "27183  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "34391  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "56723  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "56752  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "63365  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "65794  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "70860  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "75932  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "80405  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "81995  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "82765  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "97581  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "97610  /home/ubuntu/SnowData/CopernicusData/ASO_50M_S...   \n",
       "\n",
       "                                  sentinel1_filelocation  \\\n",
       "15710  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "15739  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "27183  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "34391  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "56723  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "56752  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "63365  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "65794  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "70860  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "75932  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "80405  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "81995  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "82765  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "97581  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "97610  /home/ubuntu/SnowData/Sen1_Data_poly/ASO_50M_S...   \n",
       "\n",
       "                                 sentinel2a_filelocation  \\\n",
       "15710  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "15739  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "27183  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "34391  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "56723  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "56752  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "63365  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "65794  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "70860  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "75932  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "80405  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "81995  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "82765  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "97581  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "97610  /home/ubuntu/SnowData/Sen2_DataA_poly/ASO_50M_...   \n",
       "\n",
       "                                 sentinel2b_filelocation SWE_Scaled  \n",
       "15710  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.000000  \n",
       "15739  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.110631  \n",
       "27183  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.096953  \n",
       "34391  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.187064  \n",
       "56723  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.000000  \n",
       "56752  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.021183  \n",
       "63365  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.186333  \n",
       "65794  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.034188  \n",
       "70860  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.000000  \n",
       "75932  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.000000  \n",
       "80405  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.000000  \n",
       "81995  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.030853  \n",
       "82765  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.000128  \n",
       "97581  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.182002  \n",
       "97610  /home/ubuntu/SnowData/Sen2_DataB_poly/ASO_50M_...   0.017050  \n",
       "\n",
       "[15 rows x 23 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['cell_id'] == 'ASO_50M_SWE_USCALB_34']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1212"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class args:\n",
    "    #Overall Args\n",
    "    folder_name = \"/home/ubuntu/SnowData\"\n",
    "\n",
    "    #Keep track of features used in wandb\n",
    "    features = feature_cols\n",
    "\n",
    "    #Setting the number of CPU workers we are using\n",
    "    num_workers = 12\n",
    "\n",
    "    #Setting the seed so we can replicate\n",
    "    seed = 1212\n",
    "\n",
    "    #Toggle for whether or not we want our model pretrained on imagenet\n",
    "    pretrained = True\n",
    "\n",
    "    #Next we pick the model name with the appropriate shape, img size and output\n",
    "    model_name1 = 'mixnet_s'\n",
    "    model_shape1 = 1536\n",
    "    model_name2 = 'tf_efficientnet_b2_ns'\n",
    "    model_shape2 = 1408 #768 for swin small 1536 for swin large 1792 for efficientnet b4 768 for cait-m-36\n",
    "    imagesize = 224\n",
    "    num_classes = 1\n",
    "    img_channels = 3\n",
    "\n",
    "    #LSTM variables\n",
    "    lstm_hidden = 64\n",
    "    lstm_layers = 1\n",
    "    lstm_seqlen = 10\n",
    "\n",
    "    #Training Args\n",
    "    train_batch_size = 24\n",
    "    val_batch_size = 24\n",
    "    test_batch_size = 24\n",
    "\n",
    "    #Max epochs and number of folds\n",
    "    max_epochs = 100\n",
    "    n_splits = 2\n",
    "\n",
    "    #Optimizer and Scheduler args\n",
    "    loss = 'nn.BCEWithLogitsLoss'\n",
    "    lr = 3e-4\n",
    "    warmup_epochs = 5\n",
    "    weight_decay = 3e-6\n",
    "    eta_min = 0.000001\n",
    "    n_accumulate = 1\n",
    "    T_0 = 25\n",
    "    T_max = 2000\n",
    "\n",
    "    #Callback args\n",
    "    #Minimum number amount of improvement to not trigger patience\n",
    "    min_delta = 0.0\n",
    "    #Number of epochs in a row to wait for improvement\n",
    "    patience = 30\n",
    "\n",
    "#Dataloader Args\n",
    "loaderargs = {'num_workers' : args.num_workers, 'pin_memory': False, 'drop_last': False}\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets are how pytorch knows how to read in the data\n",
    "class SWEDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df,ts, test = False, seq_len = 10):\n",
    "        self.df = df\n",
    "        self.seq_len = seq_len\n",
    "        #First we must specify the path to the images\n",
    "        #self.MOD10A1_file_names = df['MOD10A1_filelocations'].values\n",
    "        #self.MYD10A1_file_names = df['MYD10A1_filelocations'].values\n",
    "        self.copernicus_file_names = df['copernicus_filelocations'].values\n",
    "        self.sentinel1_file_names = df['sentinel1_filelocation'].values\n",
    "        self.sentinel2a_file_names = df['sentinel2a_filelocation'].values\n",
    "        self.sentinel2b_file_names = df['sentinel2b_filelocation'].values\n",
    "        #Variables to query time series and output\n",
    "        self.cell_id = df['cell_id'].values\n",
    "        self.date = df['date'].values\n",
    "        self.timeseries = ts\n",
    "        #The only transform we want to do right now is the resizing\n",
    "        self._transform = T.Resize(size= (args.imagesize, args.imagesize))\n",
    "        #We specify the tabular feature columns\n",
    "        self.meta = df[feature_cols].values\n",
    "        #Now we specify the targets\n",
    "        self.targets = df['SWE_Scaled'].values\n",
    "        #Finally we specify if this is training or test\n",
    "        self.test = test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #Get the image, scale it to between 0-1 and resize it\n",
    "        copernicus_img_path = self.copernicus_file_names[index]\n",
    "        copernicus_img = read_image(copernicus_img_path,mode = torchvision.io.image.ImageReadMode.RGB) / 255\n",
    "        copernicus_img = self._transform(copernicus_img)\n",
    "        \n",
    "        sentinel1_img_path = self.sentinel1_file_names[index]\n",
    "        sentinel1_img = read_image(sentinel1_img_path, mode = torchvision.io.image.ImageReadMode.RGB) / 255\n",
    "        sentinel1_img = self._transform(sentinel1_img)\n",
    "        \n",
    "        sentinel2a_img_path = self.sentinel2a_file_names[index]\n",
    "        sentinel2a_img = read_image(sentinel2a_img_path, mode = torchvision.io.image.ImageReadMode.RGB) / 255\n",
    "        sentinel2a_img = self._transform(sentinel2a_img)\n",
    "        \n",
    "        sentinel2b_img_path = self.sentinel2b_file_names[index]\n",
    "        sentinel2b_img = read_image(sentinel2b_img_path, mode = torchvision.io.image.ImageReadMode.RGB) / 255\n",
    "        sentinel2b_img = self._transform(sentinel2b_img)\n",
    "\n",
    "        #Pull from weather data and generate time-series\n",
    "        date_range = pd.date_range(end=self.date[index], periods=self.seq_len)\n",
    "        ts = []\n",
    "        for date in date_range:\n",
    "            query = self.timeseries.loc[(self.timeseries['cell_id']==self.cell_id[index]) & (self.timeseries['date']==date)]\n",
    "            if not query.empty:\n",
    "                tmp = list(query['HRRR_TMP_surface_12h'])[0]\n",
    "                prate = list(query['HRRR_PRATE_surface_12h'])[0]\n",
    "                if np.isnan(tmp):\n",
    "                    tmp = -1\n",
    "                if np.isnan(prate):\n",
    "                    prate = -1\n",
    "                ts.append([tmp,prate])\n",
    "            else:\n",
    "                #-1 for missing values as Nan was causing issues, and 0 is a valid value\n",
    "                ts.append([-1,-1])\n",
    "        ts = torch.tensor(ts,dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "        #Pull in the features for our batch\n",
    "        meta = self.meta[index, :]\n",
    "        \n",
    "        #Specify the target based on whether this is training or test\n",
    "        if self.test:\n",
    "          target = 0\n",
    "        else:\n",
    "          target = self.targets[index]\n",
    "            \n",
    "        return copernicus_img, sentinel1_img, sentinel2a_img, sentinel2b_img, target, meta , ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch Lightning Requires that the dataset be formatted as a module\n",
    "class SWEDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, traindf, valdf,ts,args, loaderargs):\n",
    "        super().__init__()\n",
    "        #Import our training and validation set, which we will define later\n",
    "        self._train_df = traindf\n",
    "        self._val_df = valdf\n",
    "        self.ts = ts\n",
    "\n",
    "        #Makesure we bring in our args so we can use them\n",
    "        self.args = args\n",
    "        self.loaderargs = loaderargs\n",
    "\n",
    "    #Building the datasets\n",
    "    def __create_dataset(self, train=True):\n",
    "        if train == 'train':\n",
    "          return SWEDataset(self._train_df,self.ts)\n",
    "        else:\n",
    "          return SWEDataset(self._val_df, self.ts)\n",
    "\n",
    "    #Using the datasets to return a dataloader\n",
    "    def train_dataloader(self):\n",
    "        SWE_train = self.__create_dataset(\"train\")\n",
    "        return DataLoader(SWE_train, **self.loaderargs, batch_size=self.args.train_batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        SWE_val = self.__create_dataset(\"val\")\n",
    "        return DataLoader(SWE_val, **self.loaderargs, batch_size=self.args.val_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_transforms():\n",
    "    transform = {\n",
    "        \"train\": T.Compose(\n",
    "            [\n",
    "                #T.RandomHorizontalFlip(),\n",
    "                #T.RandomVerticalFlip(),\n",
    "                #T.RandomAffine(15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "                T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "                T.ConvertImageDtype(torch.float),\n",
    "                T.Normalize(mean = (0.485, 0.456, 0.406), \n",
    "                            std = (0.229, 0.224, 0.225))\n",
    "                \n",
    "            ]\n",
    "        ),\n",
    "        \"val\": T.Compose(\n",
    "            [\n",
    "                T.ConvertImageDtype(torch.float),\n",
    "                T.Normalize(mean = (0.485, 0.456, 0.406), \n",
    "                            std = (0.229, 0.224, 0.225))\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "    return transform\n",
    "  \n",
    "\n",
    "def mixup(x1: torch.Tensor, x2: torch.Tensor, x3: torch.Tensor,x4: torch.Tensor,\n",
    "          y: torch.Tensor, \n",
    "          z = torch.Tensor, alpha: float = 1.0):\n",
    "    assert alpha > 0, \"alpha should be larger than 0\"\n",
    "    assert x1.size(0) > 1, \"Mixup cannot be applied to a single instance.\"\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    rand_index = torch.randperm(x1.size()[0])\n",
    "    mixed_x1 = lam * x1 + (1 - lam) * x1[rand_index, :]\n",
    "    mixed_x2 = lam * x2 + (1 - lam) * x2[rand_index, :]\n",
    "    mixed_x3 = lam * x3 + (1 - lam) * x3[rand_index, :]\n",
    "    mixed_x4 = lam * x4 + (1 - lam) * x4[rand_index, :]\n",
    "    mixed_meta = lam * z + (1 - lam) * z[rand_index, :]\n",
    "    target_a, target_b = y, y[rand_index]\n",
    "    return mixed_x1,mixed_x2,mixed_x3, mixed_x4,mixed_meta, target_a, target_b,  lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "class CNNLSTM(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.scaler = target_scaler\n",
    "        self.tabular_columns = tabluar_columns\n",
    "        self._criterion = eval(self.args.loss)()\n",
    "        self.transform = get_default_transforms()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=.2)\n",
    "        \n",
    "        self.hidden_size = 2\n",
    "        \n",
    "        #Tracking\n",
    "        self.trainr2 = R2Score()\n",
    "        self.valr2 = R2Score()\n",
    "        \n",
    "        #Image Models\n",
    "        self.model1 = timm.create_model(args.model_name1, \n",
    "                                       pretrained=args.pretrained, \n",
    "                                       num_classes=0,\n",
    "                                       in_chans = 3,\n",
    "                                       #global_pool=''\n",
    "                                       )\n",
    "        self.model2 = timm.create_model(args.model_name2, \n",
    "                                       pretrained=args.pretrained, \n",
    "                                       num_classes=0,\n",
    "                                       in_chans = 3,\n",
    "                                       #global_pool=''\n",
    "                                       )\n",
    "        self.model3 = timm.create_model(args.model_name2, \n",
    "                                       pretrained=args.pretrained, \n",
    "                                       num_classes=0,\n",
    "                                       in_chans = 3,\n",
    "                                       #global_pool=''\n",
    "                                       )\n",
    "        self.model4 = timm.create_model(args.model_name2, \n",
    "                                       pretrained=args.pretrained, \n",
    "                                       num_classes=0,\n",
    "                                       in_chans = 3,\n",
    "                                       #global_pool=''\n",
    "                                       )\n",
    "        #LSTM\n",
    "        self.lstm = nn.LSTM(input_size = 2,\n",
    "                            hidden_size = args.lstm_hidden,\n",
    "                            num_layers = args.lstm_layers,\n",
    "                            batch_first=True,dropout=.1)\n",
    "        #Possible multiple LSTM layers?\n",
    "        #self.lstm = nn.LSTM(input_size = args.lstm_hidden,\n",
    "        #                    hidden_size = self.hidden_size,\n",
    "        ##                    num_layers = args.lstm_layers,\n",
    "        #                    batch_first=True,dropout=.1)\n",
    "        \n",
    "        #Linear regression layer\n",
    "        self.linear1 = nn.Linear(6415,1024)\n",
    "        self.linear2 = nn.Linear(1024,256)\n",
    "        self.linear3 = nn.Linear(256,args.num_classes)\n",
    "    \n",
    "        \n",
    "    def forward(self,features1,features2,features3,features4,meta,ts):\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Image Convolution\n",
    "        #Image Models\n",
    "        features1 = self.model1(features1)                 \n",
    "        features1 = self.relu(features1)\n",
    "        features1 = self.dropout(features1)\n",
    "        \n",
    "        features2 = self.model2(features2)                 \n",
    "        features2 = self.relu(features2)\n",
    "        features2 = self.dropout(features2)\n",
    "        \n",
    "        features3 = self.model3(features3)                 \n",
    "        features3 = self.relu(features3)\n",
    "        features3 = self.dropout(features3)\n",
    "        \n",
    "        features4 = self.model4(features4)                 \n",
    "        features4 = self.relu(features4)\n",
    "        features4 = self.dropout(features4)\n",
    "        \n",
    "\n",
    "        #LSTM\n",
    "        batch_size, seq_len, feature_len = ts.size()\n",
    "        # Initialize hidden state with zeros\n",
    "        \n",
    "        h_0 = torch.zeros(1, batch_size, 64,requires_grad=True).cuda()\n",
    "        c_0 = torch.zeros(1, batch_size, 64,requires_grad=True).cuda()\n",
    "        \n",
    "        f_ts, (final_hidden,final_cell) = self.lstm(ts, (h_0,c_0))\n",
    "        f_ts = f_ts.contiguous().view(batch_size,-1)\n",
    "        \n",
    "        #*************************************************************\n",
    "        #Concatenate meta and image features\n",
    "        features = torch.cat([features1,features2,features3,features4,f_ts,meta],dim=1)\n",
    "        #*************************************************************\n",
    "        \n",
    "        #Linear\n",
    "        features = self.linear1(features)\n",
    "        features = self.relu(features)\n",
    "        features = self.dropout(features)\n",
    "        \n",
    "        features = self.linear2(features)\n",
    "        features = self.relu(features)\n",
    "        features = self.dropout(features)\n",
    "        \n",
    "        output = self.linear3(features)\n",
    "        return output\n",
    "    \n",
    "###I DIDN\"T MIX UP TS data\n",
    "    def __share_step(self, batch, mode):\n",
    "        copernicus_img, sentinel1_img, sentinel2a_img, sentinel2b_img, labels, meta,ts = batch\n",
    "        labels = labels.float()\n",
    "        meta = meta.float()\n",
    "        ts = ts.float()\n",
    "        copernicus_img = self.transform[mode](copernicus_img)\n",
    "        sentinel1_img = self.transform[mode](sentinel1_img)\n",
    "        sentinel2a_img = self.transform[mode](sentinel2a_img)\n",
    "        sentinel2b_img = self.transform[mode](sentinel2b_img)\n",
    "\n",
    "        rand_index = torch.rand(1)[0]\n",
    "        \n",
    "        #This is a mixup function\n",
    "        if rand_index < 0.5 and mode == 'train':\n",
    "            copernicus_mixed,sentinel1_mixed,sentinel2a_mixed,sentinel2b_mixed, mixed_meta, target_a, target_b, lam = mixup(\n",
    "                                                          copernicus_img,sentinel1_img,sentinel2a_img,sentinel2b_img,\n",
    "                                                          labels, meta, alpha=0.5)\n",
    "            logits = self.forward(copernicus_mixed,sentinel1_mixed,sentinel2a_mixed,sentinel2b_mixed, mixed_meta,ts).squeeze(1)\n",
    "            loss = self._criterion(logits, target_a) * lam + \\\n",
    "                (1 - lam) * self._criterion(logits, target_b)\n",
    "\n",
    "        else:  \n",
    "          logits = self.forward(copernicus_img,sentinel1_img,sentinel2a_img,sentinel2b_img, meta,ts).squeeze(1)\n",
    "          loss = self._criterion(logits, labels)\n",
    "\n",
    "        pred = torch.from_numpy(self.scaler \\\n",
    "            .inverse_transform(np.array(logits.sigmoid().detach().cpu()) \\\n",
    "            .reshape(-1, 1)))\n",
    "        labels = torch.from_numpy(self.scaler \\\n",
    "            .inverse_transform(np.array(labels.detach().cpu()) \\\n",
    "            .reshape(-1, 1)))\n",
    "        \n",
    "        '''\n",
    "        #This is random noise\n",
    "        elif rand_index > 0.8 and mode == 'train':\n",
    "            images = images + (torch.randn(images.size(0),3,args.imagesize,args.imagesize, \n",
    "                                           dtype = torch.float, device = device)*10)/100\n",
    "            logits = self.forward(images, meta).squeeze(1)\n",
    "            loss = self._criterion(logits, labels)\n",
    "        '''\n",
    "\n",
    "        return loss, pred, labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, pred, labels = self.__share_step(batch, 'train')\n",
    "        self.trainr2(pred.cuda(),labels.cuda())\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        return {'loss': loss, 'pred': pred, 'labels': labels}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, pred, labels = self.__share_step(batch, 'val')\n",
    "        self.valr2(pred.cuda(),labels.cuda())\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return {'pred': pred, 'labels': labels}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        self.log('train_r2_epoch',self.trainr2)\n",
    "        self.__share_epoch_end(outputs, 'train')\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        self.log('val_r2_epoch',self.valr2)\n",
    "        self.__share_epoch_end(outputs, 'val')\n",
    "\n",
    "    def __share_epoch_end(self, outputs, mode):\n",
    "        preds = []\n",
    "        labels = []\n",
    "        for out in outputs:\n",
    "            pred, label = out['pred'], out['labels']\n",
    "            preds.append(pred)\n",
    "            labels.append(label)\n",
    "        preds = torch.cat(preds)\n",
    "        labels = torch.cat(labels)\n",
    "        metrics = torch.sqrt(((labels - preds) ** 2).mean())\n",
    "        self.log(f'{mode}_RMSE', metrics)    \n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=args.lr, weight_decay = args.weight_decay)\n",
    "        \n",
    "        return {\n",
    "        \"optimizer\": optimizer,\n",
    "        \"lr_scheduler\": {\n",
    "            \"scheduler\": CosineAnnealingLR(optimizer, T_max = args.T_max, eta_min= args.eta_min),\n",
    "            \"interval\": \"step\",\n",
    "            \"monitor\": \"train_loss\",\n",
    "            \"frequency\": 1}\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msnowcastshowdown\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/2022_snowpack_capstone/notebooks/models/wandb/run-20220325_203929-3w1ud48i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/snowcastshowdown/ASO_Modeling/runs/3w1ud48i\" target=\"_blank\">T-sepimage-estv-cnn-lstm</a></strong> to <a href=\"https://wandb.ai/snowcastshowdown/ASO_Modeling\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name       | Type              | Params\n",
      "--------------------------------------------------\n",
      "0  | _criterion | BCEWithLogitsLoss | 0     \n",
      "1  | relu       | ReLU              | 0     \n",
      "2  | dropout    | Dropout           | 0     \n",
      "3  | trainr2    | R2Score           | 0     \n",
      "4  | valr2      | R2Score           | 0     \n",
      "5  | model1     | EfficientNet      | 2.6 M \n",
      "6  | model2     | EfficientNet      | 7.7 M \n",
      "7  | model3     | EfficientNet      | 7.7 M \n",
      "8  | model4     | EfficientNet      | 7.7 M \n",
      "9  | lstm       | LSTM              | 17.4 K\n",
      "10 | linear1    | Linear            | 6.6 M \n",
      "11 | linear2    | Linear            | 262 K \n",
      "12 | linear3    | Linear            | 257   \n",
      "--------------------------------------------------\n",
      "32.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "32.6 M    Total params\n",
      "130.203   Total estimated model params size (MB)\n",
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/ubuntu/snowcap/weights exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7f3a806c44482a9277069c78e928eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168939651d0240ca8ffb6ce3a3cda945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Not doing kfold, instead separating by year\n",
    "traindf = df[(pd.to_datetime(df['date']).dt.year == int('2019'))|\n",
    "            (pd.to_datetime(df['date']).dt.year == int('2018'))].copy().reset_index(drop=True)\n",
    "valdf = df[(pd.to_datetime(df['date']).dt.year == int('2016'))|\n",
    "            (pd.to_datetime(df['date']).dt.year == int('2017'))].copy().reset_index(drop=True)\n",
    "\n",
    "model = CNNLSTM()\n",
    "\n",
    "modelname = 'T-sepimage-estv-cnn-lstm'\n",
    "\n",
    "#Callbacks\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_RMSE\", min_delta=args.min_delta, patience=args.patience, \n",
    "                                    verbose=False, mode=\"min\")\n",
    "progressbar = TQDMProgressBar(refresh_rate = 10)\n",
    "checkpoint_callback = ModelCheckpoint(dirpath='/home/ubuntu/snowcap/weights', \n",
    "                                      filename= f\"{modelname}_best_weights\", save_top_k=1, monitor=\"val_RMSE\")\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "#Initialize wandb()\n",
    "wandb.init(name=modelname,project = \"ASO_Modeling\", entity = \"snowcastshowdown\", job_type='train')\n",
    "\n",
    "#Log model parameters into wandb (args variable dictionary)\n",
    "args_dict = dict(args.__dict__)\n",
    "#pop out non-json-able variables\n",
    "for key in ['__module__','__dict__','__weakref__','__doc__']:\n",
    "    args_dict.pop(key,None)\n",
    "wandb.config.update(args_dict)\n",
    "\n",
    "\n",
    "wandb_logger = WandbLogger(log_model = 'all')\n",
    "\n",
    "wandb_logger.watch(model)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=args.max_epochs, \n",
    "                    gpus=1, \n",
    "                    logger=wandb_logger,\n",
    "                    callbacks=[early_stop_callback, \n",
    "                                progressbar, \n",
    "                                checkpoint_callback,\n",
    "                                lr_monitor])\n",
    "\n",
    "SWE_Datamodule = SWEDataModule(traindf, valdf, weather, args = args, loaderargs = loaderargs)\n",
    "\n",
    "trainer.fit(model, SWE_Datamodule)\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.137 MB of 0.137 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–â–â–â–â–</td></tr><tr><td>lr-AdamW</td><td>â–ˆâ–ˆâ–‡â–‡â–†â–…â–„â–‚â–</td></tr><tr><td>train_loss_step</td><td>â–ˆâ–ƒâ–â–ƒâ–‚â–â–â–ƒâ–‚</td></tr><tr><td>trainer/global_step</td><td>â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>lr-AdamW</td><td>0.00026</td></tr><tr><td>train_loss_step</td><td>0.42593</td></tr><tr><td>trainer/global_step</td><td>449</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">sepimage-cnn-lstm</strong>: <a href=\"https://wandb.ai/snowcastshowdown/ASO_Modeling/runs/j1jm9s5h\" target=\"_blank\">https://wandb.ai/snowcastshowdown/ASO_Modeling/runs/j1jm9s5h</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220323_184320-j1jm9s5h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
